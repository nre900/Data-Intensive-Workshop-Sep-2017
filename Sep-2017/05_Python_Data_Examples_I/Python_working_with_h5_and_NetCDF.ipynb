{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../nci-logo.png)\n",
    "\n",
    "-------\n",
    "# Data Access and Manipulation Using iPython Notebooks\n",
    "## Working with HDF and netCDF files\n",
    "\n",
    "\n",
    "\n",
    "### In this notebook:\n",
    "\n",
    "- Using iPython Notebooks with NetCDF and h5 files within the VDI\n",
    "    - <a href='#part1'>Launch Jupyter Notebook</a>  \n",
    "    - <a href='#part2'>Importing Python libraries</a>  \n",
    "    - <a href='#part3'>How to write HDF and netCDF files</a> \n",
    "    - <a href='#part4'>Opening file and viewing contents</a>\n",
    "    - <a href='#part5'>Command-line tools for HDF5 files</a>\n",
    "    - <a href='#part6'>NCO basics</a>\n",
    "    \n",
    "---------\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part1'></a> \n",
    "## Launch the Jupyter Notebook application\n",
    "\n",
    "#### Using pre-built VDI modules:\n",
    "\n",
    "Load the following modules:\n",
    "\n",
    "```\n",
    "    $ module load python/2.7.11\n",
    "    $ module load python/2.7.11-matplotlib\n",
    "    $ module load ipython/4.2.0-py2.7\n",
    "    $ module load netcdf/4.3.3.1\n",
    "    $ module load netcdf4-python/1.2.4-ncdf-4.3.3.1-py2.7\n",
    "    $ module load h5py/2.6.0-hdf5-1.8.14p-py2.7\n",
    "    $ module load nco/4.5.3\n",
    "```    \n",
    "    \n",
    "<br>\n",
    "Launch the Jupyter Notebook application:\n",
    "```\n",
    "    $ jupyter notebook\n",
    "``` \n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<b>NOTE: </b> This will launch the <b>Notebook Dashboard</b> within a new web browser window. \n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Using virtual environments:\n",
    "\n",
    "To use along with customised python packages in a virtual environment, begin by following the steps in **Python on the VDI: Part II**. \n",
    "\n",
    "Once you have a virtual environment setup with your packages (including `Jupyter`), proceed by loading the required modules and activating the virtual environment:\n",
    "\n",
    "```\n",
    "    $ module load python/2.7.11\n",
    "    $ source <path_to_virtual_environment>/bin/activate\n",
    "```\n",
    "\n",
    "<br>\n",
    "Then, as above, launch the Jupyter Notebook application:\n",
    "\n",
    "```\n",
    "    $ jupyter notebook\n",
    "```    \n",
    "    \n",
    "<div class=\"alert alert-warning\">\n",
    "<b>NOTE: </b> If you have already followed <b>Python on the VDI: Part II</b>, you should have installed the netcdf4-python package, which is required in the remainder of this notebook.  \n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part2'></a> \n",
    "## Import python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from netCDF4 import Dataset\n",
    "import time\n",
    "from numpy.random import uniform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part3'></a> \n",
    "## How to write HDF5 and NetCDF files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDF (Heirarchical Data Format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first work on creating a HDF5 (Heirarchical Data Format) file. HDF5 is a powerful binary data format with no upper limit on the file size. It provides parallel IO and runs under the hood low level optimisations to make queries faster and storage requirements smaller. HDF5 files work generally like standard Python file objects. They support standard modes like r/w/a, and should be closed when they are no longer in use.\n",
    "\n",
    "We have to initialise our HDF5 file using <span style=\"color:red\">h5py.File</span> and providing the arguments of filename and mode. As we are writing this file, we provide a __w__ for write access:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h5file = h5py.File(\"testfile.hdf5\", \"w\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file name may be a byte string or unicode string. \n",
    "\n",
    "| Valid modes        | Description           |\n",
    "| ------------- |:-------------:|\n",
    "| r   | Readonly, file must exist |\n",
    "| r+      | Read/write, file must exist      |\n",
    "| w | Create file, truncate if exists      |\n",
    "| w- or x | Create file, fail if exists |\n",
    "| a | Read/write if exists, create otherwise (default)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create an HDF5 dataset. Datasets are very similar to NumPy arrays in that they are homogeneous collections of data elements, with an immutable datatype and (hyper)rectangular shape. HDF5 datasets support a variety of transparent storage features (e.g. compression, error-dection and chunked I/O)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New datasets are created using either <span style=\"color:red\">Group.create_dataset()</span> or <span style=\"color:red\">Group.require_dataset()</span>. To make an empty dataset, one must specify a name, shape and (optionally) the data type (default is 'f'):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " dset1 = h5file.create_dataset(\"Zxx\", (1000,), dtype='i')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HDF5 datasets have both a shape and data type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dset1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dset1.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to initialise the dataset to an existing NumPy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "initdata = np.arange(0,1,0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dset2 = h5file.create_dataset(\"Zxy\", data=initdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dset3 = h5file.create_dataset(\"Zyx\", data=initdata, dtype='f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HDF5 datasets are by default contiguous. However, datasets can be created using HDF5's chunked storage layout. This means the dataset is divided up into regularly-sized pieces which are stored haphazardly on disk, and indexed using a B-tree.\n",
    "\n",
    "Chunked storage makes it possible to resize datasets, allowing compression filters. To enable chunked storage, set the keyword <span style=\"color:red\">chunk</span> to a tuple indicating the chunk shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dset4 = h5file.create_dataset(\"Zyy\", (1000,1000), chunks=(100,100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, data will be read and written in blocks with shape (100,100). Since picking a chunk shape can be confusing, h5py can guess a chunk shape for you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dset5 = h5file.create_dataset(\"Zyyb\", (1000,1000), chunks=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print dset1.shape, dset2.shape, dset3.shape, dset4.shape, dset5.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Groups and heirarchical organisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every object in an HDF5 file has a name, and they're arranged in a POSIX-style hierarchy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dset3.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"folders\" in this system are called <span style=\"color:red\">groups</span>. Let's now create a subgroup using <span style=\"color:red\">create_group</span> and add some other datasets to this subgroup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "group1 = h5file.create_group(\"seismic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dsetseis1 = group1.create_dataset(\"some_seismic_data\", (50,), dtype = 'f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dsetseis1.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can specify the full path when creating the datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataseis2 = h5file.create_dataset(\"subgroup2/some_more_seismic_data\", (10,), dtype='i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataseis1a = h5file.create_dataset(\"seismic/some_more_seismic\", (10,), dtype='i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataseis1a.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can retrieve objects in the file using the item-relevant syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myfileseis = h5file['seismic/some_more_seismic']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's list all the groups in our file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for name in h5file:\n",
    "    print name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One great feature of HDF5 is that you can store metadata right next to the data it describes. All groups and datasets support attached named bits of data called *attributes*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dset1.attrs['latitude'] = 32.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dset1.attrs['longitude'] = 144.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'longitude' in dset1.attrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All we need to do now is close the file, which will write all our work to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h5file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NetCDF (Network Common Data Form)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a netCDF file. For this we are going to use the netCDF4-python module. To create a netCDF file from python, use the  <span style=\"color:red\">Dataset()</span> constructor. This method is also used to open an existing netCDF file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_nCDF = Dataset('geophys.nc','w',format='NETCDF4_CLASSIC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print dataset_nCDF.file_format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a set of dimensions used for your variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "period = dataset_nCDF.createDimension('period', 20)\n",
    "apparent_resistivity = dataset_nCDF.createDimension('apparent_resistivity',503)\n",
    "lat = dataset_nCDF.createDimension('lat', 73)\n",
    "lon = dataset_nCDF.createDimension('lon', 144)\n",
    "time = dataset_nCDF.createDimension('time', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print len(lon), len(time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Dimensions__: All of the *Dimension* instances are stored in a python dictionary. Therefore, we can access each dimension by its name using dictionary key access: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print 'Lon dimension:', dataset_nCDF.dimensions['lon']\n",
    "print 'Lat dimension:', dataset_nCDF.dimensions['lat']\n",
    "print 'Period dimension:', dataset_nCDF.dimensions['period']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for dimname in dataset_nCDF.dimensions.keys():\n",
    "    dim = dataset_nCDF.dimensions[dimname]\n",
    "    print dimname, len(dim), dim.isunlimited()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Variables__: NetCDF variables behave much like python multi-dimensional arrays in numpy. However, unlike numpy arrays, netCDF variables can be appended to along the *unlimited* dimension. To create a netCDF variable, use <span style=\"color:red\">Dataset.createVariable(*var_id*, *type*, *dimensions*)</span>:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "times = dataset_nCDF.createVariable('time', np.float64, ('time',))\n",
    "latitudes = dataset_nCDF.createVariable('latitude', np.float32, ('lat',))\n",
    "longitudes = dataset_nCDF.createVariable('longitude', np.float32, ('lon',))\n",
    "periods = dataset_nCDF.createVariable('period', np.float32, ('period',))\n",
    "\n",
    "# create the actual 4-D variable\n",
    "app_resistivities = dataset_nCDF.createVariable('apparent_resistivity', np.float32, ('time','lat','lon','period'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "app_resistivities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the variables in the *Dataset* are stored in a Python dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print 'lat variable:', dataset_nCDF.variables['latitude']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for varname in dataset_nCDF.variables.keys():\n",
    "    var = dataset_nCDF.variables[varname]\n",
    "    print varname, var.dtype, var.dimensions, var.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Attributes (global)__: Global attributes are set by assigning values to *Dataset* instance variables. Attributes can be strings, numbers or sequences.\n",
    "\n",
    "__Attributes (variable)__: Variable attributes are set by assigning to *Variable* instance variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Global Attributes\n",
    "\n",
    "dataset_nCDF.description = 'some test EM data'\n",
    "dataset_nCDF.history = 'Created'+time.ctime(time.time())\n",
    "dataset_nCDF.source = 'netCDF4 python module tutorial'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Variable attributes\n",
    "\n",
    "latitudes.units='degree_north'\n",
    "longitudes.units='degree_east'\n",
    "times.units='hours since 0001-01-01 00:00:00'\n",
    "times.calendar='gregorian'\n",
    "periods.units='seconds'\n",
    "app_resistivities.units='ohm.m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print dataset_nCDF.description\n",
    "print dataset_nCDF.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Writing Data__: To put data into our netCDF Variables, we can assign data to a slice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lats = np.arange(-90,90,2.5)\n",
    "lons = np.arange(-180,180,2.5)\n",
    "periods = np.arange(0.01,100,5)\n",
    "latitudes[:] = lats\n",
    "longitudes[:] = lons\n",
    "periods[:] = periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print 'latitudes =\\n', latitudes[:]\n",
    "print 'longitudes =\\n', longitudes[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NetCDF *variable* objects that have an unlimited dimension will grow along that dimension if you assign data outside the currently defined range of indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print 'app_resistivity shape before adding data =', app_resistivities.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlats = len(dataset_nCDF.dimensions['lat'])\n",
    "nlons = len(dataset_nCDF.dimensions['lon'])\n",
    "nperiods = len(dataset_nCDF.dimensions['period'])\n",
    "app_resistivities[1:10,:,:,:] = uniform(size=(9,nlats,nlons,nperiods))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Time coordinates__: Most metadata standards (e.g. CF, COARDS) specify that time be measured relative to a fixed date using a certain calendar (e.g. hours since YY:MM:DD hh-mm-ss\"). The functions <span style=\"color:red\">num2date()</span> and <span style=\"color:red\">date2num()</span> can be used to convert values to and from calendar dates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from netCDF4 import num2date, date2num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dates = []\n",
    "\n",
    "for n in range(app_resistivities.shape[0]):\n",
    "    dates.append(datetime(2000,1,1)+ n*timedelta(hours=12))\n",
    "\n",
    "times[:] = date2num(dates, units = times.units, calendar = times.calendar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print 'time values (in units %s):'% times.units + '\\n', times[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dates = num2date(times[:], units = times.units, calendar=times.calendar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print 'dates corresponding to time values:\\n', dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Finally, we need to write the file:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_nCDF.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the file is written."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part4'></a> \n",
    "## Opening file and viewing contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first begin with a HDF file. For this example we will use the same file we previously created. To open and read data, use the <span style=\"color:red\">File</span> method in read mode, *r*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myfile = h5py.File('testfile.hdf5','r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what data is in the file by using the call <span style=\"color:red\">keys()</span> on the file object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myfile.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can grab some of the datasets we created using the <span style=\"color:red\">get</span> method and specifying the dataset name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d1 = myfile.get('Zxy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d2 = myfile.get('Zyy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see all the groups and datasets in our HDF file by running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_groups = [obj for obj in myfile if isinstance(myfile[obj],h5py.Group)]\n",
    "all_datasets = [obj for obj in myfile if isinstance(myfile[obj],h5py.Dataset)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print all_groups, all_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what exists in the group *seismic*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myfile[\"seismic\"].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can grab a dataset within *seismic*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d3 = myfile[\"seismic\"].get('some_seismic_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__d1__, __d2__ and __d3__ are HDF5 dataset objects. To convert these into arrays, use numpy's array method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d1a = np.array(d1)\n",
    "d1a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d2a = np.array(d2)\n",
    "d2a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d3a = np.array(d3)\n",
    "d3a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d1a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d2a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d3a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's open and view the contents of the netCDF file we created in the previous section. To open a netCDF file from python, call the <span style=\"color:red\"> Dataset()</span> constructor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "netcdf_dataset = Dataset('geophys.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print netcdf_dataset.file_format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interrogate dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print netcdf_dataset.dimensions.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print netcdf_dataset.dimensions['lat']\n",
    "print netcdf_dataset.dimensions['time']\n",
    "print netcdf_dataset.dimensions['apparent_resistivity']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interrogate variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print netcdf_dataset.variables.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print netcdf_dataset.variables['latitude']\n",
    "print netcdf_dataset.variables['time']\n",
    "print netcdf_dataset.variables['apparent_resistivity']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interrogate global and variable attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find all netCDF global attributes\n",
    "\n",
    "for attr in netcdf_dataset.ncattrs():\n",
    "    print attr, '=', getattr(netcdf_dataset,attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find variable attributes\n",
    "\n",
    "netcdf_dataset.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "netcdf_dataset.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part5'></a> \n",
    "## Command-line tools for HDF5 files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are numerous command-line tools included in the HDF5 distribution to view, edit, convert and compare HDF5 files. Let's use our testfile.hdf5 file for the following examples. We will begin with <span style=\"color:red\">h5dump</span>, which enables the user to examine the contents of an HDF5 and dump those contents to an ASCII file: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h5dump -n testfile.hdf5 # -n displays a list of the objects in a file\n",
    "\n",
    "h5dump -H testfile.hdf5 # displays the header information only (no data)\n",
    "\n",
    "h5dump -d \"/seismic/some_seismic_data\" testfile.hdf5 # display a specific dataset.\n",
    "\n",
    "h5dump -d \"seismic/some_seismic_data\" -o seismic.txt -y testfile.hdf5 # converts specified dataset to ASCII file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">h5stat</span> can be used to print statistics about HDF5 files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h5stat testfile.hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a complete list of HDF5 tools available, see the HDF5 tools page https://support.hdfgroup.org/HDF5/doc/RM/Tools.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='part6'></a> \n",
    "## NCO basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__NCO (NetCDF Operators)__ is a suite of command-line based tools designed to facilitate manipulation and analysis of self-describing data stored in the netCDF-accessible formats, including DAP, HDF4 and HDF5. Let's begin with the <span style=\"color:red\">ncks</span> (netCDF kitchen sink) operator. This command can give an overview of a netCDF file, extract certain variables, extract certain dimensions and manipulate record dimensions. Let's use our example geophys.nc file for this example and open up a terminal.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# View the contents of a netCDF file\n",
    "\n",
    "ncks geophys.nc | more\n",
    "\n",
    "# View a variable\n",
    "\n",
    "ncks -v latitude geophys.nc | more\n",
    "ncks -v apparent_resistivity geophys.nc | more\n",
    "\n",
    "# View multiple variables\n",
    "\n",
    "ncks -v longitudetime geophys.nc | more\n",
    "\n",
    "# View a variable over some dimension subsets\n",
    "\n",
    "ncks -v apparent_resistivity -d lat,5,7 -d period,1,2 -d lon,55,57 geophys.nc | more "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">ncks</span> can also output data from an input file into an output file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ncks -v latitude,longitude geophys.nc -O grid.nc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ncks -v apparent_resistivity -d lat,5,7 -d period,1,2 -d lon,55,57 geophys.nc -O appres.nc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can look at the <span style=\"color:red\">ncrcat</span> and <span style=\"color:red\">ncecat</span> commands. These concatinate multiple files together into a single file. Use <span style=\"color:red\">ncrcat</span> when there is a record dimension (e.g. concatinating multiple daily files into one monthly file) and <span style=\"color:red\">ncecat</span> when there in no record dimension - a new record dimension will be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ncrcat file1 file2 -O outputfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test <span style=\"color:red\">ncrcat</span> and <span style=\"color:red\">ncecat</span> on some files you are interested in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other operators include:\n",
    "- __ncap2__: netCDF Arithmetic Processor\n",
    "- __ncatted__: netCDF Attribute Editor\n",
    "- __ncbo__: netCDF Binary Operator\n",
    "- __nces__: netCDF Ensemble Statistics\n",
    "- __ncflint__: netCDF File Interpolator\n",
    "- __ncra__: netCDF Record Averager\n",
    "- __ncwaa__: netCDF Weighted Averager\n",
    "\n",
    "Try these operators out on your netCDF files! For help on a particular operator, type <span style=\"color:red\"> man operator </span> (e.g. man ncbo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
